
<!doctype html>
<html>

<head>
<title>Yuqian Yuan</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Yuqian Yuan"> 
<meta name="description" content="Yuqian Yuan's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />
<link rel="stylesheet" href="css/style2.css">
<!-- <link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css"> -->
<!-- <link href="assets/css/bootstrap-responsive.min.css" rel="stylesheet" type="text/css"> -->


</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Yuqian Yuan 袁瑜谦<h1>
				</div>

                <h3>PhD Student</h3>

		<p>
                    Zhejiang University </br>
                    Email: yuanyuqian@zju.edu.cn </br>
		</p>
		<p>
			<a href="https://github.com/CircleRadon"><img src="assets/logos/github_logo.png" height="30px"></a>&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=7D7QL9MAAAAJ&hl=zh-CN"><img src="assets/logos/google_logo.png" height="30px"></a>&nbsp;&nbsp;
		</p>
			</td>

			</td>
			<td width="20%">
				<img src="assets/imgs/me3.jpg" width="120%"/>
			</td>
		<tr>
	</tbody>
</table>


<h2>About me</h2> 

<p style="line-height: 25px;">
I am currently a PhD student in Zhejiang University, advised by Prof. <a href="https://scholar.google.com/citations?user=XBbuP9YAAAAJ&hl=zh-CN">Wenqiao Zhang</a> and <a href="https://scholar.google.com/citations?user=fqOwFhQAAAAJ&hl=zh-CN">Jun Xiao</a>. 
My recent research interests are <em>multimodal large language models</em>, <em>image&video understanding and reasoning</em>. 
Before, I mainly focus on 
the field of the techniques for object detection, image segmentaion under minimal human supervision, including label-efficient /weakly-supervised /un-supervised approaches.<br>
</p>

<h2>News</h2> 
<ul>
    <li>
        [2025.6]: We released the <a href="https://circleradon.github.io/EOCBench/"> EOC-Bench </a>, an object-centric embodied cognition benchmark in dynamic egocentric scenarios.
    </li>
    <li>
        [2025.5]: One paper, TokenPacker is accepted by <b>IJCV 2025</b>.
    </li>
    <li>
        [2025.4]: Our VideoRefer and VideoRefer-Bench have been discussed and adopted by NVIDIA & UC Berkely in their <a herf="https://arxiv.org/pdf/2504.16072">DAM</a> work.
    </li>
    <li>
        [2025.2]: Two papers are accepted by <b>CVPR 2025</b>.
    </li>
    <li>
        [2025.2]: We released the <a herf="https://huggingface.co/datasets/DAMO-NLP-SG/VideoRefer-700K">VideoRefer-700K</a> dataset on HuggingFace. Please see the <a href="https://github.com/DAMO-NLP-SG/VideoRefer">VideoRefer Suite</a> for the details.
    </li>
    <li>
        [2025.1]: We released <a herf="https://arxiv.org/abs/2501.13106">VideoLLaMA3</a>, frontier multimodal foundation models for both image and video understanding.
    </li>
</ul>


<p id="publications">
<h2>Publications&Preprints</h2>
</p>


<div id="pubs"></div>
    
<div id="pubs"></div>
    <div class="paper" style="clear:left;">
    <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
        <img src="assets/imgs/eocbench.png" width="210" class="img-bordered" alt="photo">
    </div>
    <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?
    </div>
    <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        <b>Yuqian Yuan*</b>, Ronghao Dang*, Long Li*, Wentong Li*, Diao Jiao, Xin Li, Deli Zhao, Fan Wang, Wenqiao Zhang, Jun Xiao, Yueting Zhuang
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        Arxiv, 2025
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        <p>
        [<a href="https://arxiv.org/abs/2506.05287" target="_blank" rel="noopener">Paper</a>]
        [<a href="https://circleradon.github.io/EOCBench/" target="_blank" rel="noopener">Project Page</a>]
        [<a href="https://github.com/alibaba-damo-academy/EOCBench" target="_blank" rel="noopener">Code</a>]
        [<a href="https://huggingface.co/datasets/CircleRadon/EOC-Bench" target="_blank" rel="noopener">HuggingFace</a>]
        [<a href="https://circleradon.github.io/EOCBench/#leaderboard" target="_blank" rel="noopener">LeaderBoard</a>]
        </p>
    </div>
    </div>
    
    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/videorefer.gif" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            <b>Yuqian Yuan</b>, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, Jianke Zhu, Lidong Bing
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            CVPR, 2025
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/abs/2501.00599" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/DAMO-NLP-SG/VideoRefer" target="_blank" rel="noopener">code</a>] 
            [<a href="https://damo-nlp-sg.github.io/VideoRefer/" target="_blank" rel="noopener">project page</a>] 
            </p>
        </div>
    </div>

    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/ecbench.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Ronghao Dang*, <b>Yuqian Yuan*</b>, Wenqi Zhang*, Yifei Xin, Boqiang Zhang, Long Li, Liuyi Wang, Qinyang Zeng, Xin Li, Lidong Bing
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            CVPR, 2025
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/abs/2501.05031" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/Rh-Dang/ECBench" target="_blank" rel="noopener">code</a>] 
            [<a href="https://rh-dang.github.io/ECBench/" target="_blank" rel="noopener">project page</a>] 
            </p>
        </div>
    </div>

    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/tokenpacker.jpg" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            TokenPacker: Efficient Visual Projector for Multimodal LLM 
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Wentong Li*, <b>Yuqian Yuan*</b>, Jian Liu, Dongqi Tang, Song Wang, Jianke Zhu, Lei Zhang
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            IJCV, 2025
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/abs/2407.02392" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/CircleRadon/TokenPacker" target="_blank" rel="noopener">code</a>] 
            [<a href="https://zhuanlan.zhihu.com/p/707021763" target="_blank" rel="noopener">知乎</a>] 
            </p>
        </div>
    </div>


    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/videollama3.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding 
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Boqiang Zhang*, Kehan Li*, Zesen Cheng*, Zhiqiang Hu*, <b>Yuqian Yuan*</b>, Guanzheng Chen*, Sicong Leng*, Yuming Jiang*, Hang Zhang*, Xin Li*, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, Deli Zhao
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            Technical Report, 2025
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/abs/2501.13106" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/DAMO-NLP-SG/VideoLLaMA3" target="_blank" rel="noopener">code</a>] 
            </p>
        </div>
    </div>

    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/healthgpt.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation 
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Tianwei Lin, Wenqiao Zhang, Sijing Li, <b>Yuqian Yuan</b>, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            ICML, 2025 (<b>Spotlight</b>)
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/abs/2502.09838" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/DCDmllm/HealthGPT" target="_blank" rel="noopener">code</a>] 
            </p>
        </div>
    </div>

    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/qyqx.gif" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            Osprey: Pixel Understanding with Visual Instruction Tuning 
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            <b>Yuqian Yuan*</b>, Wentong Li*, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, Jianke Zhu
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            CVPR, 2024
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/pdf/2312.10032.pdf" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/CircleRadon/Osprey" target="_blank" rel="noopener">code</a>] 
            [<a href="https://www.youtube.com/watch?v=YsxqHBBnDfk" target="_blank" rel="noopener">video demo</a>] 
            [<a href="https://zhuanlan.zhihu.com/p/673647000" target="_blank" rel="noopener">知乎</a>] 
            </p>
        </div>
    </div>



    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/apro.gif" width="200" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        Label-efficient Segmentation via Affinity Propagation
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        Wentong Li*, <b>Yuqian Yuan*</b>, Song Wang, Wenyu Liu, Dongqi Tang, Jian Liu, Jianke Zhu, Lei Zhang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        NeurIPS, 2023
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          [<a href="https://arxiv.org/pdf/2310.10533.pdf" target="_blank" rel="noopener">paper</a>]
          [<a href="https://github.com/CircleRadon/APro" target="_blank" rel="noopener">code</a>] 
	  [<a href="https://liwentomng.github.io/apro/" target="_blank" rel="noopener">project page</a>]
          [<a href="https://zhuanlan.zhihu.com/p/674018681" target="_blank" rel="noopener">知乎</a>] 
          </p>
      </div>
    </div>
           
           
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:7px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/point2mask.png" width="200" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
          Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
          Wentong Li, <b>Yuqian Yuan</b>, Song Wang, Jianke Zhu, Jianshu Li, Jian Liu, Lei Zhang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          ICCV, 2023
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        <p>
        [<a href="https://arxiv.org/abs/2308.01779" target="_blank" rel="noopener">paper</a>]
        [<a href="https://github.com/LiWentomng/Point2Mask" target="_blank" rel="noopener">code</a>] 
        </p>
    </div>
    </div>

<!-- </ul> -->
</script>


</script>

<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>


<div>
    <h2>Research Intern</h2>
    <!-- 列表区域 -->
    <ul style="list-style: none; padding: 0; margin: 0;">
        <!-- 单个列表项 -->
        <li style="display: flex; align-items: center; margin-bottom: 20px;">
            <!-- 左边的图片 -->
            <div style="flex-shrink: 0; width: 100px; height: 100%; margin-right: 20px;">
                <img src="assets/imgs/damo.png" alt="Alibaba Icon" style="width: 100%; height: 100%; object-fit: cover;">
            </div>
            <!-- 右边的内容 -->
            <div>
                Alibaba DAMO Academy | Hangzhou | Jul.2024 - Present<br>
                Topic: Video understanding with MLLM, Embodied AI<br>
                Mentor: <a href="https://lixin4ever.github.io/">Xin Li</a>, <a href="https://lidongbing.github.io/">Lidong Bing</a>
            </div>
        </li>

        <li style="display: flex; align-items: center; margin-bottom: 20px;">
            <!-- 左边的图片 -->
            <div style="flex-shrink: 0; width: 100px; height: 100%; margin-right: 20px;">
                <img src="assets/imgs/ant.png" alt="Ant Group Icon" style="width: 100%; height: 100%; object-fit: cover;">
            </div>
            <!-- 右边的内容 -->
            <div>
                Ant Group | Hangzhou | Apr.2024 - Jul.2024<br>
                Topic: Image understanding with MLLM, Image Tampering Detection and Segmentation
            </div>
        </li>
    </ul>
</div>

<h2>Honors</h2>

<ul>
    <li>
        National Scholarship, 2021
    </li>
    <li>
        Silver Medal, China Collegiate Programming Contest for Girls, 2021, 2020
    </li>
    <li>
        Honorable Mention, The 45th ICPC Asia Regional Contest, 2021
    </li>
    <li>
        Best Girl's Team, Jiangsu Collegiate Programming Contest, 2021
    </li>
    <li>
        The 17th place, China Collegiate Programming Contest for Girls, 2020
    </li>
    <li>
        Second Prize, The 11th "Blue Bridge Cup" National Software Competition Final, 2020
    </li>
</ul>



<table width="100%"> 
	<tr> 
		<td align="center">&copy; Yuqian Yuan | Last update: May 2025</td>
	</tr> 
</table>

</div>


</body>

</html>

