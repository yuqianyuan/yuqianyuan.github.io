
<!doctype html>
<html>

<head>
<title>Yuqian Yuan</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Yuqian Yuan"> 
<meta name="description" content="Yuqian Yuan's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />
<link rel="stylesheet" href="css/style2.css">
<!-- <link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css"> -->
<!-- <link href="assets/css/bootstrap-responsive.min.css" rel="stylesheet" type="text/css"> -->


</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Yuqian Yuan 袁瑜谦<h1>
				</div>

                <h3>PhD Student</h3>

		<p>
                    Zhejiang University </br>
                    Email: yuanyuqian@zju.edu.cn </br>
		</p>
		<p>
			<a href="https://github.com/CircleRadon"><img src="assets/logos/github_logo.png" height="30px"></a>&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=7D7QL9MAAAAJ&hl=zh-CN"><img src="assets/logos/google_logo.png" height="30px"></a>&nbsp;&nbsp;
		</p>
			</td>

			</td>
			<td width="20%">
				<img src="assets/imgs/me3.jpg" width="120%"/>
			</td>
		<tr>
	</tbody>
</table>


<h2>About me</h2> 

<p style="line-height: 25px;">
  I'm currently a PhD student in Zhejiang University, advised by 
  Prof. <a href="https://scholar.google.com/citations?user=XBbuP9YAAAAJ&hl=zh-CN">Wenqiao Zhang</a> 
  and Prof. <a href="https://scholar.google.com/citations?user=fqOwFhQAAAAJ&hl=zh-CN">Jun Xiao</a>. 
  Previously, I was also advised by 
  Prof. <a href="https://scholar.google.com/citations?user=SC-WmzwAAAAJ&hl=zh-CN">Jianke Zhu</a>.
  My research focuses on advancing <em>Visual Understanding</em>, 
  <em>Vision-Language Models</em> and <em>Embodied AI</em>, particularly in:
</p>

<ul style="line-height: 25px; margin-top: 0; margin-bottom: 0; padding-left: 20px;">
  <li style="margin: 2px 0;">
    <strong><em>Fine-grained spatial-temporal understanding with VLMs:</em></strong>
    <a href="https://arxiv.org/abs/2510.23603">PixelRefer</a>, 
    <a href="https://arxiv.org/abs/2501.00599">VideoRefer</a>, 
    <a href="https://arxiv.org/abs/2312.10032">Osprey</a>
  </li>
  <li style="margin: 2px 0;">
    <strong><em>General VLMs for image/video understanding:</em></strong>
    <a href="https://arxiv.org/abs/2501.13106">VideoLLaMA 3</a>, 
    <a href="https://arxiv.org/abs/2407.02392">TokenPacker</a>
  </li>
  <li style="margin: 2px 0;">
    <strong><em>Embodied spatial understanding &amp; reasoning &amp; action:</em></strong>
    <a href="https://arxiv.org/abs/2602.14979v1">RynnBrain</a>, 
    <a href="https://arxiv.org/abs/2511.17502">RynnVLA-002</a>, 
    <a href="https://arxiv.org/abs/2508.14160">RynnEC</a>, 
    <a href="https://arxiv.org/abs/2506.05287">EOC-Bench</a>, 
    <a href="https://arxiv.org/abs/2501.05031">ECBench</a>
  </li>
</ul>
<p style="line-height: 25px;">
Before, I mainly focus on the field of the techniques for object detection, image segmentaion under minimal human supervision, including label-efficient /weakly-supervised /un-supervised approaches.
</p>

<h2>News</h2> 
<ul>
    <li>
        [2026.2]: We released <a href="https://github.com/alibaba-damo-academy/RynnBrain">RynnBrain</a>, an embodied foundation model grounded in physical reality.
    </li>
    <li>
        [2025.11]: We released <a href="https://github.com/alibaba-damo-academy/RynnVLA-002">RynnVLA-002</a>, a unified vision-language-action and world model.
    </li>
    <li>
        [2025.10]: We released <a href="https://github.com/alibaba-damo-academy/PixelRefer">PixelRefer</a>, a new unified pixel-level MLLM framework for fine-grained regional understanding.
    </li>
    <li>
        [2025.9]: Our <a href="https://circleradon.github.io/EOCBench/"> EOC-Bench </a> is accepted by <b>NeurIPS 2025</b>.
    </li>
    <li>
        [2025.8]: We released <a href="https://github.com/alibaba-damo-academy/RynnEC"> RynnEC </a>, a video MLLM specifically designed for embodied cognition tasks.
    </li>
    <li>
        [2025.6]: We released the <a href="https://circleradon.github.io/EOCBench/"> EOC-Bench </a>, an object-centric embodied cognition benchmark in dynamic egocentric scenarios.
    </li>
    <li>
        [2025.5]: One paper, TokenPacker is accepted by <b>IJCV 2025</b>.
    </li>
    <li>
        [2025.4]: Our VideoRefer and VideoRefer-Bench have been discussed and adopted by NVIDIA & UC Berkely in their <a herf="https://arxiv.org/pdf/2504.16072">DAM</a> work.
    </li>
    <li>
        [2025.2]: Two papers are accepted by <b>CVPR 2025</b>.
    </li>
    <li>
        [2025.2]: We released the <a herf="https://huggingface.co/datasets/DAMO-NLP-SG/VideoRefer-700K">VideoRefer-700K</a> dataset on HuggingFace. Please see the <a href="https://github.com/DAMO-NLP-SG/VideoRefer">VideoRefer Suite</a> for the details.
    </li>
    <li>
        [2025.1]: We released <a herf="https://arxiv.org/abs/2501.13106">VideoLLaMA3</a>, frontier multimodal foundation models for both image and video understanding.
    </li>
</ul>


<p id="publications">
<h2>Publications&Preprints</h2>
</p>


<div id="pubs"></div>
<div class="paper" style="clear:left;">
    <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
        <img src="assets/imgs/rynnbrain.png" width="210" class="img-bordered" alt="photo">
    </div>
    <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        RynnBrain: Open Embodied Foundation Models
    </div>
    <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        Ronghao Dang*, Jiayan Guo*, Bohan Hou*, Sicong Leng*, Kehan Li*, Xin Li*, Jiangpin Liu*, Yunxuan Mao*, Zhikai Wang*, <b>Yuqian Yuan*</b>, Minghao Zhu*, Xiao Lin, Yang Bai, Qian Jiang, Yaxi Zhao, Minghua Zeng, Junlong Gao, Yuming Jiang, Jun Cen, Siteng Huang, Liuyi Wang, Wenqiao Zhang, Chengju Liu, Jianfei Yang, Shijian Lu, Deli Zhao (Core contributors in alphabetical order)
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        Technical Report, 2025
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        <p>
        [<a href="https://arxiv.org/pdf/2602.14979v1" target="_blank" rel="noopener">Paper</a>]
        [<a href="https://github.com/alibaba-damo-academy/RynnBrain" target="_blank" rel="noopener">Code</a>]
        [<a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io" target="_blank" rel="noopener">Project Page</a>]
        <img src="https://img.shields.io/github/stars/alibaba-damo-academy/RynnBrain?style=social" />
        </p>
    </div>
    </div>

<div id="pubs"></div>
<div class="paper" style="clear:left;">
    <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
        <img src="assets/imgs/rynnvla002.png" width="210" class="img-bordered" alt="photo">
    </div>
    <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        RynnVLA-002: A Unified Vision-Language-Action and World Model
    </div>
    <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        Jun Cen*, Siteng Huang*, <b>Yuqian Yuan*</b>, Kehan Li*, Hangjie Yuan, Chaohui Yu, Yuming Jiang, Jiayan Guo, Xin Li, Hao Luo, Fan Wang, Deli Zhao, Hao Chen
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        Arxiv, 2025
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        <p>
        [<a href="https://arxiv.org/pdf/2511.17502" target="_blank" rel="noopener">Paper</a>]
        [<a href="https://github.com/alibaba-damo-academy/RynnVLA-002" target="_blank" rel="noopener">Code</a>]
        <img src="https://img.shields.io/github/stars/alibaba-damo-academy/RynnVLA-002?style=social" />
        </p>
    </div>
    </div>

<div class="paper" style="clear:left;">
    <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
        <img src="assets/imgs/pixelrefer.png" width="210" class="img-bordered" alt="photo">
    </div>
    <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity
    </div>
    <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        <b>Yuqian Yuan</b>, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, Beng Chin Ooi
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        Arxiv, 2025
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        <p>
        [<a href="https://arxiv.org/pdf/2510.23603" target="_blank" rel="noopener">Paper</a>]
        [<a href="https://github.com/alibaba-damo-academy/PixelRefer" target="_blank" rel="noopener">Code</a>]
        [<a href="https://huggingface.co/spaces/lixin4ever/PixelRefer" target="_blank" rel="noopener">Demo</a>]
        <img src="https://img.shields.io/github/stars/alibaba-damo-academy/PixelRefer?style=social" />
        </p>
    </div>
    </div>

<div class="paper" style="clear:left;">
    <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
        <img src="assets/imgs/rynnec.png" width="210" class="img-bordered" alt="photo">
    </div>
    <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        RynnEC: Bringing MLLMs into Embodied World
    </div>
    <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        Ronghao Dang*, <b>Yuqian Yuan*</b>, Yunxuan Mao*, Kehan Li*, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, Deli Zhao
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        Technical Report, 2025
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        <p>
        [<a href="https://arxiv.org/abs/2508.14160" target="_blank" rel="noopener">Paper</a>]
        [<a href="https://github.com/alibaba-damo-academy/RynnEC" target="_blank" rel="noopener">Code</a>]
        [<a href="https://huggingface.co/blog/Alibaba-DAMO-Academy/rynnec" target="_blank" rel="noopener">Blog</a>]
        [<a href="https://huggingface.co/spaces/Alibaba-DAMO-Academy/RynnEC" target="_blank" rel="noopener">Demo</a>]
        [<a href="https://www.youtube.com/watch?v=vsMxbzsmrQc" target="_blank" rel="noopener">Video</a>]
        <img src="https://img.shields.io/github/stars/alibaba-damo-academy/RynnEC?style=social" />
        </p>
    </div>
    </div>

    <div class="paper" style="clear:left;">
    <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
        <img src="assets/imgs/eocbench.png" width="210" class="img-bordered" alt="photo">
    </div>
    <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?
    </div>
    <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        <b>Yuqian Yuan*</b>, Ronghao Dang*, Long Li*, Wentong Li*, Diao Jiao, Xin Li, Deli Zhao, Fan Wang, Wenqiao Zhang, Jun Xiao, Yueting Zhuang
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        NeurIPS, 2025
    </div>
    <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        <p>
        [<a href="https://arxiv.org/abs/2506.05287" target="_blank" rel="noopener">Paper</a>]
        [<a href="https://circleradon.github.io/EOCBench/" target="_blank" rel="noopener">Project Page</a>]
        [<a href="https://github.com/alibaba-damo-academy/EOCBench" target="_blank" rel="noopener">Code</a>]
        [<a href="https://huggingface.co/datasets/CircleRadon/EOC-Bench" target="_blank" rel="noopener">HuggingFace</a>]
        [<a href="https://circleradon.github.io/EOCBench/#leaderboard" target="_blank" rel="noopener">LeaderBoard</a>]
        </p>
    </div>
    </div>
    
    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/videorefer.gif" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            <b>Yuqian Yuan</b>, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, Jianke Zhu, Lidong Bing
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            CVPR, 2025
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/abs/2501.00599" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/DAMO-NLP-SG/VideoRefer" target="_blank" rel="noopener">code</a>] 
            [<a href="https://damo-nlp-sg.github.io/VideoRefer/" target="_blank" rel="noopener">project page</a>] 
            <img src="https://img.shields.io/github/stars/DAMO-NLP-SG/VideoRefer?style=social" />
            </p>
        </div>
    </div>

    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/ecbench.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Ronghao Dang*, <b>Yuqian Yuan*</b>, Wenqi Zhang*, Yifei Xin, Boqiang Zhang, Long Li, Liuyi Wang, Qinyang Zeng, Xin Li, Lidong Bing
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            CVPR, 2025
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/abs/2501.05031" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/Rh-Dang/ECBench" target="_blank" rel="noopener">code</a>] 
            [<a href="https://rh-dang.github.io/ECBench/" target="_blank" rel="noopener">project page</a>] 
            </p>
        </div>
    </div>

    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/tokenpacker.jpg" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            TokenPacker: Efficient Visual Projector for Multimodal LLM 
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Wentong Li*, <b>Yuqian Yuan*</b>, Jian Liu, Dongqi Tang, Song Wang, Jianke Zhu, Lei Zhang
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            IJCV, 2025
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/abs/2407.02392" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/CircleRadon/TokenPacker" target="_blank" rel="noopener">code</a>] 
            [<a href="https://zhuanlan.zhihu.com/p/707021763" target="_blank" rel="noopener">知乎</a>] 
            <img src="https://img.shields.io/github/stars/CircleRadon/TokenPacker?style=social" />
            </p>
        </div>
    </div>


    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/videollama3.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding 
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Boqiang Zhang*, Kehan Li*, Zesen Cheng*, Zhiqiang Hu*, <b>Yuqian Yuan*</b>, Guanzheng Chen*, Sicong Leng*, Yuming Jiang*, Hang Zhang*, Xin Li*, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, Deli Zhao
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            Technical Report, 2025
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/abs/2501.13106" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/DAMO-NLP-SG/VideoLLaMA3" target="_blank" rel="noopener">code</a>] 
            <img src="https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA3?style=social" />
            </p>
        </div>
    </div>

    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/healthgpt.png" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation 
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            Tianwei Lin, Wenqiao Zhang, Sijing Li, <b>Yuqian Yuan</b>, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            ICML, 2025 (<b>Spotlight</b>)
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/abs/2502.09838" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/DCDmllm/HealthGPT" target="_blank" rel="noopener">code</a>]
            <img src="https://img.shields.io/github/stars/DCDmllm/HealthGPT?style=social" /> 
            </p>
        </div>
    </div>

    <div class="paper" style="clear:left;">
        <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
            <img src="assets/imgs/qyqx.gif" width="200" class="img-bordered" alt="photo">
        </div>
        <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
            Osprey: Pixel Understanding with Visual Instruction Tuning 
        </div>
        <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
            <b>Yuqian Yuan*</b>, Wentong Li*, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, Jianke Zhu
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            CVPR, 2024
        </div>
        <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
            <p>
            [<a href="https://arxiv.org/pdf/2312.10032.pdf" target="_blank" rel="noopener">paper</a>]
            [<a href="https://github.com/CircleRadon/Osprey" target="_blank" rel="noopener">code</a>] 
            [<a href="https://www.youtube.com/watch?v=YsxqHBBnDfk" target="_blank" rel="noopener">video demo</a>] 
            [<a href="https://zhuanlan.zhihu.com/p/673647000" target="_blank" rel="noopener">知乎</a>] 
            <img src="https://img.shields.io/github/stars/CircleRadon/Osprey?style=social" /> 
            </p>
        </div>
    </div>



    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:10px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/apro.gif" width="200" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">
        Label-efficient Segmentation via Affinity Propagation
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
        Wentong Li*, <b>Yuqian Yuan*</b>, Song Wang, Wenyu Liu, Dongqi Tang, Jian Liu, Jianke Zhu, Lei Zhang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        NeurIPS, 2023
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          <p>
          [<a href="https://arxiv.org/pdf/2310.10533.pdf" target="_blank" rel="noopener">paper</a>]
          [<a href="https://github.com/CircleRadon/APro" target="_blank" rel="noopener">code</a>] 
	  [<a href="https://liwentomng.github.io/apro/" target="_blank" rel="noopener">project page</a>]
          [<a href="https://zhuanlan.zhihu.com/p/674018681" target="_blank" rel="noopener">知乎</a>] 
          <img src="https://img.shields.io/github/stars/CircleRadon/APro?style=social" /> 
          </p>
      </div>
    </div>
           
           
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-top:7px;margin-bottom:10px;padding-left:3px;">
          <img src="assets/imgs/point2mask.png" width="200" class="img-bordered" alt="photo">
      </div>
      <div class="ptitle" style="padding:1px;margin-left:240px;line-height:1.8">	
          Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport
      </div>
      <div class="pauthors" style="padding:1px;margin-left:240px;line-height:1.8">
          Wentong Li, <b>Yuqian Yuan</b>, Song Wang, Jianke Zhu, Jianshu Li, Jian Liu, Lei Zhang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
          ICCV, 2023
      </div>
      <div class="pvenue" style="padding:1px;margin-left:240px;line-height:1.8">
        <p>
        [<a href="https://arxiv.org/abs/2308.01779" target="_blank" rel="noopener">paper</a>]
        [<a href="https://github.com/LiWentomng/Point2Mask" target="_blank" rel="noopener">code</a>] 
        <img src="https://img.shields.io/github/stars/LiWentomng/Point2Mask?style=social" /> 
        </p>
    </div>
    </div>

<!-- </ul> -->
</script>


</script>

<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>


<div>
    <h2>Research Intern</h2>
    <!-- 列表区域 -->
    <ul style="list-style: none; padding: 0; margin: 0;">
        <!-- 单个列表项 -->
        <li style="display: flex; align-items: center; margin-bottom: 20px;">
            <!-- 左边的图片 -->
            <div style="flex-shrink: 0; width: 100px; height: 100%; margin-right: 20px;">
                <img src="assets/imgs/damo.png" alt="Alibaba Icon" style="width: 100%; height: 100%; object-fit: cover;">
            </div>
            <!-- 右边的内容 -->
            <div>
                Alibaba DAMO Academy | Hangzhou | Jul.2024 - Present<br>
                Topic: Video understanding with MLLM, Embodied AI<br>
                Mentor: <a href="https://lixin4ever.github.io/">Xin Li</a>, <a href="https://lidongbing.github.io/">Lidong Bing</a>
            </div>
        </li>

        <li style="display: flex; align-items: center; margin-bottom: 20px;">
            <!-- 左边的图片 -->
            <div style="flex-shrink: 0; width: 100px; height: 100%; margin-right: 20px;">
                <img src="assets/imgs/ant.png" alt="Ant Group Icon" style="width: 100%; height: 100%; object-fit: cover;">
            </div>
            <!-- 右边的内容 -->
            <div>
                Ant Group | Hangzhou | Apr.2024 - Jul.2024<br>
                Topic: Image understanding with MLLM, Image Tampering Detection and Segmentation
            </div>
        </li>
    </ul>
</div>

<h2>Invited Talks</h2>
<ul>
    <li>
        Sept 19, 2025: REAL Lab @ ZJU. Topic: Fine-grained Pixel-level Understanding: From general scene to embodied.
    </li>
    <li>
        Jun 5, 2025: AI TIME. Topic: VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM.
    </li>
    <li>
        Jun 10, 2024: AntGroup. Topic: Frontier Multimodal Large Models.
    </li>
</ul>


<h2>Honors</h2>

<ul>
    <li>
        National Scholarship, 2021
    </li>
    <li>
        Silver Medal, China Collegiate Programming Contest for Girls, 2021, 2020
    </li>
    <li>
        Honorable Mention, The 45th ICPC Asia Regional Contest, 2021
    </li>
    <li>
        Best Girl's Team, Jiangsu Collegiate Programming Contest, 2021
    </li>
    <li>
        The 17th place, China Collegiate Programming Contest for Girls, 2020
    </li>
    <li>
        Second Prize, The 11th "Blue Bridge Cup" National Software Competition Final, 2020
    </li>
</ul>



<table width="100%"> 
	<tr> 
		<td align="center">&copy; Yuqian Yuan | Last update: Oct 2025</td>
	</tr> 
</table>

</div>


</body>

</html>

